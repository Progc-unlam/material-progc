{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prueba 0 - Hola Mundo GPU.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZPZQY8W3nSJ"
      },
      "source": [
        "# Características del GPU\n",
        "\n",
        "El objetivo del ejemplo es mostrar las caracteristicas del GPU que disponibiliza colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhBpCo9K35Bo"
      },
      "source": [
        "## Listar el tipo de GPU que esta activa en el cuaderno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGyORtGk3lzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51744947-e785-47a5-f17e-abe0397a3558"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-d7c19abf-9992-15ea-19ff-7d7cccf652ac)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-C-hC5L6btL"
      },
      "source": [
        "## Listar máxima cantidad de Grillas /Bloques soportados por el GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftrYmQJ76grO",
        "outputId": "3b29ca39-1aef-43f1-fa4d-3ffe08007500"
      },
      "source": [
        "!rm -rf cuda-samples\n",
        "!git clone https://github.com/NVIDIA/cuda-samples.git\n",
        "! cd cuda-samples/Samples/1_Utilities/deviceQuery; nvcc deviceQuery.cpp -I ../../../Common -o deviceQuery\n",
        "!echo \"------------------------------------------------------------------- \"\n",
        "!cuda-samples/Samples/1_Utilities/deviceQuery/deviceQuery"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cuda-samples'...\n",
            "remote: Enumerating objects: 28487, done.\u001b[K\n",
            "remote: Counting objects: 100% (13959/13959), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1522/1522), done.\u001b[K\n",
            "remote: Total 28487 (delta 13269), reused 12437 (delta 12437), pack-reused 14528 (from 2)\u001b[K\n",
            "Receiving objects: 100% (28487/28487), 135.47 MiB | 17.31 MiB/s, done.\n",
            "Resolving deltas: 100% (24982/24982), done.\n",
            "Updating files: 100% (2241/2241), done.\n",
            "------------------------------------------------------------------- \n",
            "cuda-samples/Samples/1_Utilities/deviceQuery/deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version / Runtime Version          12.4 / 12.5\n",
            "  CUDA Capability Major/Minor version number:    7.5\n",
            "  Total amount of global memory:                 15095 MBytes (15828320256 bytes)\n",
            "  (040) Multiprocessors, (064) CUDA Cores/MP:    2560 CUDA Cores\n",
            "  GPU Max Clock rate:                            1590 MHz (1.59 GHz)\n",
            "  Memory Clock rate:                             5001 Mhz\n",
            "  Memory Bus Width:                              256-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total shared memory per multiprocessor:        65536 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  1024\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Managed Memory:                Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.4, CUDA Runtime Version = 12.5, NumDevs = 1\n",
            "Result = PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kS3hk4Q9Fiw"
      },
      "source": [
        "# Ejemplo Hola Mundo con GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se instala el módulo de cuda para python"
      ],
      "metadata": {
        "id": "SDwEganZ-WxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNfSj31s-Vs3",
        "outputId": "6367e450-1d28-45f4-c059-0aab7f2c6788"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2025.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2025.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pycuda) (4.3.8)\n",
            "Requirement already satisfied: mako in /usr/lib/python3/dist-packages (from pycuda) (1.1.3)\n",
            "Collecting siphash24>=1.6 (from pytools>=2011.2->pycuda)\n",
            "  Downloading siphash24-1.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from pytools>=2011.2->pycuda) (4.13.2)\n",
            "Downloading pytools-2025.1.6-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading siphash24-1.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2025.1-cp311-cp311-linux_x86_64.whl size=660424 sha256=1fb457a35a0636a9d22fc8917cbc2ea8ea9e364fa261d06e58a83b08a4dbed69\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/7e/6c/d2d1451ea6424cdc3d67b36c16fa7111eafdf2034bc3405666\n",
            "Successfully built pycuda\n",
            "Installing collected packages: siphash24, pytools, pycuda\n",
            "Successfully installed pycuda-2025.1 pytools-2025.1.6 siphash24-1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guhdN7HtNF_4"
      },
      "source": [
        "## Reinicio el buffer de plataforma Colab, donde la GPU escribe en lugar de la consola."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KvAZprl-Rvl"
      },
      "source": [
        "!>/var/colab/app.log"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6i2-QUgOfL6"
      },
      "source": [
        "## Ejecuto el ejemplo Hola Mundo\n",
        "\n",
        "Se puede demotrar el comportamiento de la forma de planificación de hilos. Ademas que el kernel ahora soporta la función printf()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PKA9cn79Jxg",
        "outputId": "85f200db-86ff-4661-e8bf-279c1dadaae1"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# --------------------------------------------\n",
        "#@title Parámetros de ejecución { vertical-output: true }\n",
        "\n",
        "cantidad_N =   15#@param {type: \"number\"}\n",
        "# --------------------------------------------\n",
        "import numpy\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "pycuda.tools.make_default_context()\n",
        "\n",
        "# CPU - Defino la función kernel que ejecutará en GPU.\n",
        "module = SourceModule(\"\"\"\n",
        "#include <stdio.h>\n",
        "__global__ void kernel_HolaMundo( int N )\n",
        "{\n",
        "  int idx = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "\n",
        "  if( idx < N )\n",
        "  {\n",
        "    printf( \"Hola Mundo desde el GPU - idx %d, Bloque id %d, Thread id %d \\\\n \", idx, blockIdx.x, threadIdx.x );\n",
        "  }\n",
        "  else\n",
        "  {\n",
        "    printf( \"No saludo, porque soy un hilo planificado de mas - idx %d \\\\n \", idx );\n",
        "  }\n",
        "\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# CPU - Genero la función kernel.\n",
        "kernel = module.get_function(\"kernel_HolaMundo\")\n",
        "\n",
        "dim_hilo = 32\n",
        "dim_bloque = int( (cantidad_N+dim_hilo-1) / dim_hilo )\n",
        "\n",
        "#TODO: Ojo, con los tipos de las variables en el kernel.\n",
        "kernel( numpy.int32(cantidad_N),  block=( dim_hilo, 1, 1 ),grid=(dim_bloque, 1,1) )\n",
        "\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print( \"Hola Mundo desde el CPU => Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola Mundo desde el CPU => Thread x:  32 , Bloque x: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ1ZnJtKMNu0"
      },
      "source": [
        "Muestro el buffer de COLAB:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8PUKLQGLRpy",
        "outputId": "e02f2740-1744-483e-daa3-5237b04d8d45"
      },
      "source": [
        "cat /var/colab/app.log"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"Hola Mundo desde el GPU - idx 32, Bloque id 1, Thread id 0 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 0, Bloque id 0, Thread id 0 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 1, Bloque id 0, Thread id 1 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 2, Bloque id 0, Thread id 2 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 3, Bloque id 0, Thread id 3 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 4, Bloque id 0, Thread id 4 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 5, Bloque id 0, Thread id 5 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 6, Bloque id 0, Thread id 6 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 7, Bloque id 0, Thread id 7 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 8, Bloque id 0, Thread id 8 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 9, Bloque id 0, Thread id 9 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 10, Bloque id 0, Thread id 10 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 11, Bloque id 0, Thread id 11 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 12, Bloque id 0, Thread id 12 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 13, Bloque id 0, Thread id 13 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 14, Bloque id 0, Thread id 14 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 15, Bloque id 0, Thread id 15 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 16, Bloque id 0, Thread id 16 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 17, Bloque id 0, Thread id 17 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 18, Bloque id 0, Thread id 18 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 19, Bloque id 0, Thread id 19 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 20, Bloque id 0, Thread id 20 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 21, Bloque id 0, Thread id 21 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 22, Bloque id 0, Thread id 22 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 23, Bloque id 0, Thread id 23 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 24, Bloque id 0, Thread id 24 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 25, Bloque id 0, Thread id 25 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 26, Bloque id 0, Thread id 26 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 27, Bloque id 0, Thread id 27 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 28, Bloque id 0, Thread id 28 \",\"time\":\"2025-06-04T16:24:15.445Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 29, Bloque id 0, Thread id 29 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 30, Bloque id 0, Thread id 30 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 31, Bloque id 0, Thread id 31 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 33 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 34 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 35 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 36 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 37 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 38 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 39 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 40 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 41 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 42 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 43 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 44 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 45 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 46 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 47 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 48 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 49 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 50 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 51 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 52 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 53 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 54 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 55 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 56 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 57 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 58 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 59 \",\"time\":\"2025-06-04T16:24:15.446Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 60 \",\"time\":\"2025-06-04T16:24:15.447Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 61 \",\"time\":\"2025-06-04T16:24:15.447Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 62 \",\"time\":\"2025-06-04T16:24:15.447Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 63 \",\"time\":\"2025-06-04T16:24:15.447Z\",\"v\":0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Profiler en cuda."
      ],
      "metadata": {
        "id": "1sXF4Wn29yUd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx89A4WkrjdR",
        "outputId": "4f2f8d5e-ee99-4ec8-d05d-a24d6bfad5c4"
      },
      "source": [
        "!cd /usr/local/cuda/samples/0_Simple/vectorAdd/; make >/dev/null\n",
        "#!/usr/local/cuda/samples/bin/x86_64/linux/release/vectorAdd\n",
        "!/usr/local/cuda/bin/nvprof --csv --concurrent-kernels on --openmp-profiling on --print-gpu-trace --normalized-time-unit us --print-gpu-trace /usr/local/cuda/samples/bin/x86_64/linux/release/vectorAdd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Vector addition of 50000 elements]\n",
            "==346== NVPROF is profiling process 346, command: /usr/local/cuda/samples/bin/x86_64/linux/release/vectorAdd\n",
            "==346== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "Copy input data from the host memory to the CUDA device\n",
            "CUDA kernel launch with 196 blocks of 256 threads\n",
            "Copy output data from the CUDA device to the host memory\n",
            "Test PASSED\n",
            "Done\n",
            "==346== Profiling application: /usr/local/cuda/samples/bin/x86_64/linux/release/vectorAdd\n",
            "==346== Profiling result:\n",
            "\"Start\",\"Duration\",\"Grid X\",\"Grid Y\",\"Grid Z\",\"Block X\",\"Block Y\",\"Block Z\",\"Registers Per Thread\",\"Static SMem\",\"Dynamic SMem\",\"Size\",\"Throughput\",\"SrcMemType\",\"DstMemType\",\"Device\",\"Context\",\"Stream\",\"Name\",\"Correlation_ID\"\n",
            "us,us,,,,,,,,B,B,KB,GB/s,,,,,,,\n",
            "356140.739000,38.911000,,,,,,,,,,195.312500,4.786937,\"Pageable\",\"Device\",\"Tesla K80 (0)\",\"1\",\"7\",\"[CUDA memcpy HtoD]\",114\n",
            "356220.130000,28.767000,,,,,,,,,,195.312500,6.474937,\"Pageable\",\"Device\",\"Tesla K80 (0)\",\"1\",\"7\",\"[CUDA memcpy HtoD]\",115\n",
            "356281.793000,5.472000,196,1,1,256,1,1,8,0,0,,,,,\"Tesla K80 (0)\",\"1\",\"7\",\"vectorAdd(float const *, float const *, float*, int)\",116\n",
            "356312.897000,27.807000,,,,,,,,,,195.312500,6.698476,\"Device\",\"Pageable\",\"Tesla K80 (0)\",\"1\",\"7\",\"[CUDA memcpy DtoH]\",118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWVBT9nEKJjS"
      },
      "source": [
        "# Debug con  CUDA.\n",
        "\n",
        "Ver el ejemplo desde:\n",
        "https://wiki.tiker.net/PyCuda/FrequentlyAskedQuestions/#system-specific-questions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj69fUUEEf8c",
        "outputId": "6b745493-688a-488b-8121-2c4a5579a507"
      },
      "source": [
        "!cuda-gdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA (R) CUDA Debugger\n",
            "11.0 release\n",
            "Portions Copyright (C) 2007-2020 NVIDIA Corporation\n",
            "GNU gdb (GDB) 8.2\n",
            "Copyright (C) 2018 Free Software Foundation, Inc.\n",
            "License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n",
            "This is free software: you are free to change and redistribute it.\n",
            "There is NO WARRANTY, to the extent permitted by law.\n",
            "Type \"show copying\" and \"show warranty\" for details.\n",
            "This GDB was configured as \"x86_64-pc-linux-gnu\".\n",
            "Type \"show configuration\" for configuration details.\n",
            "For bug reporting instructions, please see:\n",
            "<http://www.gnu.org/software/gdb/bugs/>.\n",
            "Find the GDB manual and other documentation resources online at:\n",
            "    <http://www.gnu.org/software/gdb/documentation/>.\n",
            "\n",
            "For help, type \"help\".\n",
            "Type \"apropos word\" to search for commands related to \"word\".\n",
            "(cuda-gdb) help\n",
            "List of classes of commands:\n",
            "\n",
            "aliases -- Aliases of other commands\n",
            "breakpoints -- Making program stop at certain points\n",
            "cuda  -- CUDA commands\n",
            "data -- Examining data\n",
            "files -- Specifying and examining files\n",
            "internals -- Maintenance commands\n",
            "obscure -- Obscure features\n",
            "running -- Running the program\n",
            "stack -- Examining the stack\n",
            "status -- Status inquiries\n",
            "support -- Support facilities\n",
            "tracepoints -- Tracing of program execution without stopping the program\n",
            "user-defined -- User-defined commands\n",
            "\n",
            "Type \"help\" followed by a class name for a list of commands in that class.\n",
            "Type \"help all\" for the list of all commands.\n",
            "Type \"help\" followed by command name for full documentation.\n",
            "Type \"apropos word\" to search for commands related to \"word\".\n",
            "Command name abbreviations are allowed if unambiguous.\n",
            "(cuda-gdb) quit\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}