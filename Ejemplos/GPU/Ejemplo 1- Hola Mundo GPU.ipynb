{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prueba 0 - Hola Mundo GPU.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZPZQY8W3nSJ"
      },
      "source": [
        "# Características del GPU\n",
        "\n",
        "El objetivo del ejemplo es mostrar las caracteristicas del GPU que disponibiliza colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhBpCo9K35Bo"
      },
      "source": [
        "## Listar el tipo de GPU que esta activa en el cuaderno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGyORtGk3lzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f9294c-50a2-4c27-f3a1-b66503e47272"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-42d43691-f756-1d31-51ed-fffa80febfb9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-C-hC5L6btL"
      },
      "source": [
        "## Listar máxima cantidad de Grillas /Bloques soportados por el GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftrYmQJ76grO",
        "outputId": "0529e854-0456-4b46-8ac3-38a9e3c014f6"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/cuda-samples.git\n",
        "!cd  cuda-samples/Samples/1_Utilities/deviceQuery/; make >/dev/null\n",
        "!echo \"------------------------------------------------------------------- \"\n",
        "!cuda-samples/Samples/1_Utilities/deviceQuery/deviceQuery | grep \"Max dimension\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cuda-samples' already exists and is not an empty directory.\n",
            "------------------------------------------------------------------- \n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kS3hk4Q9Fiw"
      },
      "source": [
        "# Ejemplo Hola Mundo con GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se instala el módulo de cuda para python"
      ],
      "metadata": {
        "id": "SDwEganZ-WxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNfSj31s-Vs3",
        "outputId": "be10a85a-0cc8-4c0b-e764-82a90cdbab38"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycuda\n",
            "  Downloading pycuda-2022.2.2.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2022.1.14.tar.gz (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.4.4)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (3.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.2)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2022.2.2-cp310-cp310-linux_x86_64.whl size=661975 sha256=5fb1fa5e5b4b0c6b67ec87a09ce070d25c6b92174d0953a5313d3682c0338ad6\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/7b/06/82a395a243fce00035dea9914d92bbef0013401497d849f8bc\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2022.1.14-py2.py3-none-any.whl size=69855 sha256=d84e9f9ebf94f493a1fb8479f952e0b1caac261e37787b62769a944158b26521\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/02/16/aa2498ad7aa723a149ff7539f1918509661c0ae9d975b44b6d\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.2.4 pycuda-2022.2.2 pytools-2022.1.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guhdN7HtNF_4"
      },
      "source": [
        "## Reinicio el buffer de plataforma Colab, donde la GPU escribe en lugar de la consola."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KvAZprl-Rvl"
      },
      "source": [
        "!>/var/colab/app.log"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6i2-QUgOfL6"
      },
      "source": [
        "## Ejecuto el ejemplo Hola Mundo\n",
        "\n",
        "Se puede demotrar el comportamiento de la forma de planificación de hilos. Ademas que el kernel ahora soporta la función printf()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "_PKA9cn79Jxg",
        "outputId": "52208c56-3c39-40f8-e7a0-209dd3932e10"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# --------------------------------------------\n",
        "#@title Parámetros de ejecución { vertical-output: true }\n",
        "\n",
        "cantidad_N =   15#@param {type: \"number\"}\n",
        "# --------------------------------------------\n",
        "import numpy\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "pycuda.tools.make_default_context()\n",
        "\n",
        "# CPU - Defino la función kernel que ejecutará en GPU.\n",
        "module = SourceModule(\"\"\"\n",
        "#include <stdio.h>\n",
        "__global__ void kernel_HolaMundo( int N )\n",
        "{\n",
        "  int idx = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "\n",
        "  if( idx < N )\n",
        "  {\n",
        "    printf( \"Hola Mundo desde el GPU - idx %d, Bloque id %d, Thread id %d \\\\n \", idx, blockIdx.x, threadIdx.x );\n",
        "  }\n",
        "  else\n",
        "  {\n",
        "    printf( \"No saludo, porque soy un hilo planificado de mas - idx %d \\\\n \", idx );\n",
        "  }\n",
        "\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# CPU - Genero la función kernel.\n",
        "kernel = module.get_function(\"kernel_HolaMundo\")\n",
        "\n",
        "dim_hilo = 32\n",
        "dim_bloque = int( (cantidad_N+dim_hilo-1) / dim_hilo )\n",
        "\n",
        "#TODO: Ojo, con los tipos de las variables en el kernel.\n",
        "kernel( numpy.int32(cantidad_N),  block=( dim_hilo, 1, 1 ),grid=(dim_bloque, 1,1) )\n",
        "\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print( \"Hola Mundo desde el CPU => Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LogicError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLogicError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-376545565265>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#TODO: Ojo, con los tipos de las variables en el kernel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcantidad_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdim_hilo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_bloque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pycuda/driver.py\u001b[0m in \u001b[0;36mfunction_call\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"must specify block size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_block_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         \u001b[0mhandlers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_buf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_arg_buf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLogicError\u001b[0m: cuFuncSetBlockShape failed: invalid argument"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: module in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ1ZnJtKMNu0"
      },
      "source": [
        "Muestro el buffer de COLAB:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8PUKLQGLRpy",
        "outputId": "415ccfb9-c86a-4476-e3b3-78c7f80f861c"
      },
      "source": [
        "cat /var/colab/app.log"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"Hola Mundo desde el GPU - idx 32, Bloque id 1, Thread id 0 \",\"time\":\"2023-06-14T15:02:52.851Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 0, Bloque id 0, Thread id 0 \",\"time\":\"2023-06-14T15:02:52.851Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 1, Bloque id 0, Thread id 1 \",\"time\":\"2023-06-14T15:02:52.851Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 2, Bloque id 0, Thread id 2 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 3, Bloque id 0, Thread id 3 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 4, Bloque id 0, Thread id 4 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 5, Bloque id 0, Thread id 5 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 6, Bloque id 0, Thread id 6 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 7, Bloque id 0, Thread id 7 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 8, Bloque id 0, Thread id 8 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 9, Bloque id 0, Thread id 9 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 10, Bloque id 0, Thread id 10 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 11, Bloque id 0, Thread id 11 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 12, Bloque id 0, Thread id 12 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 13, Bloque id 0, Thread id 13 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 14, Bloque id 0, Thread id 14 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 15, Bloque id 0, Thread id 15 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 16, Bloque id 0, Thread id 16 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 17, Bloque id 0, Thread id 17 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 18, Bloque id 0, Thread id 18 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 19, Bloque id 0, Thread id 19 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 20, Bloque id 0, Thread id 20 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 21, Bloque id 0, Thread id 21 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 22, Bloque id 0, Thread id 22 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 23, Bloque id 0, Thread id 23 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 24, Bloque id 0, Thread id 24 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 25, Bloque id 0, Thread id 25 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 26, Bloque id 0, Thread id 26 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 27, Bloque id 0, Thread id 27 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 28, Bloque id 0, Thread id 28 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 29, Bloque id 0, Thread id 29 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 30, Bloque id 0, Thread id 30 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" Hola Mundo desde el GPU - idx 31, Bloque id 0, Thread id 31 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 33 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 34 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 35 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 36 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 37 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 38 \",\"time\":\"2023-06-14T15:02:52.852Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 39 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 40 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 41 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 42 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 43 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 44 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 45 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 46 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 47 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 48 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 49 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 50 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 51 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 52 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 53 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 54 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 55 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 56 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 57 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 58 \",\"time\":\"2023-06-14T15:02:52.853Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 59 \",\"time\":\"2023-06-14T15:02:52.854Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 60 \",\"time\":\"2023-06-14T15:02:52.854Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 61 \",\"time\":\"2023-06-14T15:02:52.854Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 62 \",\"time\":\"2023-06-14T15:02:52.854Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\" No saludo, porque soy un hilo planificado de mas - idx 63 \",\"time\":\"2023-06-14T15:02:52.854Z\",\"v\":0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Profiler en cuda."
      ],
      "metadata": {
        "id": "1sXF4Wn29yUd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx89A4WkrjdR",
        "outputId": "4f2f8d5e-ee99-4ec8-d05d-a24d6bfad5c4"
      },
      "source": [
        "!cd /usr/local/cuda/samples/0_Simple/vectorAdd/; make >/dev/null\n",
        "#!/usr/local/cuda/samples/bin/x86_64/linux/release/vectorAdd\n",
        "!/usr/local/cuda/bin/nvprof --csv --concurrent-kernels on --openmp-profiling on --print-gpu-trace --normalized-time-unit us --print-gpu-trace /usr/local/cuda/samples/bin/x86_64/linux/release/vectorAdd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Vector addition of 50000 elements]\n",
            "==346== NVPROF is profiling process 346, command: /usr/local/cuda/samples/bin/x86_64/linux/release/vectorAdd\n",
            "==346== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "Copy input data from the host memory to the CUDA device\n",
            "CUDA kernel launch with 196 blocks of 256 threads\n",
            "Copy output data from the CUDA device to the host memory\n",
            "Test PASSED\n",
            "Done\n",
            "==346== Profiling application: /usr/local/cuda/samples/bin/x86_64/linux/release/vectorAdd\n",
            "==346== Profiling result:\n",
            "\"Start\",\"Duration\",\"Grid X\",\"Grid Y\",\"Grid Z\",\"Block X\",\"Block Y\",\"Block Z\",\"Registers Per Thread\",\"Static SMem\",\"Dynamic SMem\",\"Size\",\"Throughput\",\"SrcMemType\",\"DstMemType\",\"Device\",\"Context\",\"Stream\",\"Name\",\"Correlation_ID\"\n",
            "us,us,,,,,,,,B,B,KB,GB/s,,,,,,,\n",
            "356140.739000,38.911000,,,,,,,,,,195.312500,4.786937,\"Pageable\",\"Device\",\"Tesla K80 (0)\",\"1\",\"7\",\"[CUDA memcpy HtoD]\",114\n",
            "356220.130000,28.767000,,,,,,,,,,195.312500,6.474937,\"Pageable\",\"Device\",\"Tesla K80 (0)\",\"1\",\"7\",\"[CUDA memcpy HtoD]\",115\n",
            "356281.793000,5.472000,196,1,1,256,1,1,8,0,0,,,,,\"Tesla K80 (0)\",\"1\",\"7\",\"vectorAdd(float const *, float const *, float*, int)\",116\n",
            "356312.897000,27.807000,,,,,,,,,,195.312500,6.698476,\"Device\",\"Pageable\",\"Tesla K80 (0)\",\"1\",\"7\",\"[CUDA memcpy DtoH]\",118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWVBT9nEKJjS"
      },
      "source": [
        "# Debug con  CUDA.\n",
        "\n",
        "Ver el ejemplo desde:\n",
        "https://wiki.tiker.net/PyCuda/FrequentlyAskedQuestions/#system-specific-questions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj69fUUEEf8c",
        "outputId": "6b745493-688a-488b-8121-2c4a5579a507"
      },
      "source": [
        "!cuda-gdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA (R) CUDA Debugger\n",
            "11.0 release\n",
            "Portions Copyright (C) 2007-2020 NVIDIA Corporation\n",
            "GNU gdb (GDB) 8.2\n",
            "Copyright (C) 2018 Free Software Foundation, Inc.\n",
            "License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n",
            "This is free software: you are free to change and redistribute it.\n",
            "There is NO WARRANTY, to the extent permitted by law.\n",
            "Type \"show copying\" and \"show warranty\" for details.\n",
            "This GDB was configured as \"x86_64-pc-linux-gnu\".\n",
            "Type \"show configuration\" for configuration details.\n",
            "For bug reporting instructions, please see:\n",
            "<http://www.gnu.org/software/gdb/bugs/>.\n",
            "Find the GDB manual and other documentation resources online at:\n",
            "    <http://www.gnu.org/software/gdb/documentation/>.\n",
            "\n",
            "For help, type \"help\".\n",
            "Type \"apropos word\" to search for commands related to \"word\".\n",
            "(cuda-gdb) help\n",
            "List of classes of commands:\n",
            "\n",
            "aliases -- Aliases of other commands\n",
            "breakpoints -- Making program stop at certain points\n",
            "cuda  -- CUDA commands\n",
            "data -- Examining data\n",
            "files -- Specifying and examining files\n",
            "internals -- Maintenance commands\n",
            "obscure -- Obscure features\n",
            "running -- Running the program\n",
            "stack -- Examining the stack\n",
            "status -- Status inquiries\n",
            "support -- Support facilities\n",
            "tracepoints -- Tracing of program execution without stopping the program\n",
            "user-defined -- User-defined commands\n",
            "\n",
            "Type \"help\" followed by a class name for a list of commands in that class.\n",
            "Type \"help all\" for the list of all commands.\n",
            "Type \"help\" followed by command name for full documentation.\n",
            "Type \"apropos word\" to search for commands related to \"word\".\n",
            "Command name abbreviations are allowed if unambiguous.\n",
            "(cuda-gdb) quit\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}